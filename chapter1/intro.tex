Quantum computing exploits non-classical phenomena described by quantum
mechanics, such as entanglement and superposition, to perform computations,
 often with considerable speedup with respect to classical computers.

Both theory and practical implementations have attracted considerable research
efforts in the last fourty years. Different physical systems may be used to represent
quantum information, ranging from the state of
particles or molecules, e.g. trapped ions~\cite{pogorelov_compact_2021}, to
small superconducting circuits~\cite{clarke_superconducting_2008}. Despite the
variety of solutions, realizability of large scale quantum machines is still an
open question, since the impact of negative phenomena, such as decoherence,
increase exponentially with the increases in state size, thus requiring more resources allocated to
perform error-correction. However, private enterprises started
either selling machines or providing cloud access to them, proving it is actually 
an emergent technology, with increasing interest among the scientific community.

The theory of quantum computing started with the development of the quantum
turing machine model~\cite{benioff_quantum_1982}. However the most common model
in use today is the quantum circuit model based on qubits, quantum gates and
measurement operations. A qubit is the basic unit of quantum information, a
two-state quantum mechanical system. The analogy with the bit is obvious, as a qubit can be in states $\kz$ and $k1$. However the state of a qubit can also be a linear combination of $\kz$ and $\ko$, also called a superposition of $\kz$ and $\ko$. For example, a qubit can be in state $\kpl$ or $\km$, that are both combination of $\kz$ and $\ko$. Interestingly, also $\kz$ and $\ko$ can be expressed as a linear combination of $\kpl$ and $\km$, and in fact  $\kz$ and $\ko$ are usually called the computational basis, and $\kpl$ and $\km$ are known as the diagonal basis of the state space. When measured in a basis, a qubit 
in superposition collapses to one of the two basis states,
with probability dependent on the linear combination. A quantum system may also evolve through unitary transformations, that are physically implemented as quantum gates on one or more qubits (potentially controlled by classical bits).

Quantum computing theory is mainly developed along two axis: algorithms and
protocols. Even though quantum computers obey the Church-Turing
thesis~\cite{nielsen_quantum_2010}, algorithms have been found that have more
than polynomial speedup over classical counterparts, e.g. Shor's algorithm for
factorization~\cite{shor_algorithms_1994} and the HHL algorithm for solving
linear systems of equations~\cite{harrow_quantum_2009}.

Quantum protocols, in particular cryptography-related protocols, have been  one of the first application 
of quantum theory to computer science. Due to lower physical
requirements with respect to algorithms, various protocols have been implemented
in real world contexts, also with large-scale commercially available physical implementations.
The prototipical case is the BB84 Quantum Key Distribution
protocol \note{\cite{shor_simple_2000}}, that has been proven unconditionally secure. As for algorithms, most protocols only require
classical control and communication. However some protocols such as Quantum Key
Distribution~\cite{poppe_practical_2004} and Quantum Leader
Election~\cite{tani_exact_2012} also require quantum communication (i.e. sending and receiving qubits).


Correctness proof are available for some algorithm, but there is no systematic approach for verifying protocols ad their application in larger systems.
As it is well known, correctness of classical communication protocols is already hard to prove at design stage, with notorious examples of vulnerabilities discovered for protocols previously considered to be secure.
be incorrect later in time. This is more-so true for quantum protocols, which must resist both quantum and classical attackers.


Process calculi have been successful in modeling classical concurrent systems with non-determinism, probability and communication. Process calculi provide systematic solutions for analyzing and verifying the correctness of new systems and protocols. We expect the same will hold also in the quantum case, which however turns out to be a challenging setting.


To model quantum protocols, a process calculus needs to handle both classical and quantum communication at once, and take into account the peculiar rules of quantum computations. Due to the so called \textit{No-cloning Theorem}, quantum information can not be duplicated, so can not be treated in the usual functional, "pass-by-value" fashion of process calculi. Besides, qubit in superposition can not be directly observed, as they "decay" in a probabilistic way when measured. Finally, a quantum process calculus must also deal with \textit{entangled qubits}, i.e. qubits that can not be described separately, one at a time, but must be considered as a whole compound system, even when they are stored in physically distant locations.

Of all the proposed model, none has emerged as universally accepted standard. Our objective is to address the extension of classical process algebra to the quantum world. For this we will compare the different proposals, with a foundational approach aiming at identifying the best notion of behavioural equivalence. We propose a novel calculus named Linear qCCS, for which we consider different definitions of bisimilarity.
Defining an appropriate bisimilarity is in fact the first step towards temporal logics, model checking and all the other tools available in the classical models.


\subsection*{A lacking standard notion of behavioural equivalence}

There is a number of proposals of quantum process calculi in the literature, often with different syntax, semantics and behavioural equivalences, even if they all model the same systems and the same protocols \cite{lalireProcessAlgebraicApproach2004, gayCommunicatingQuantumProcesses2005, fengProbabilisticBisimulationsQuantum2007, yingAlgebraQuantumProcesses2010, wangProbabilisticProcessAlgebra2019}. Of all these, the most established and developed are \textbf{QPAlg} \cite{lalireProcessAlgebraicApproach2004}, \textbf{CQP} \cite{gayCommunicatingQuantumProcesses2005} and \textbf{qCCS} \cite{fengProbabilisticBisimulationsQuantum2007}. They all differ in  a number of minor "classical" details, that unfortunately make the calculi difficult to compare: some are inspired by $\pi$-calculus, some by CCS, some employ strong bisimilarity while others employ weak or branching bisimilarity, some apply Larsen-Skou probabilistic bisimilarity \cite{larsenBisimulationProbabilisticTesting1991}, some apply Segala probabilistic bisimilarity \cite{segalaProbabilisticSimulationsProbabilistic1994}.

More importantly, the proposed languages treat quantum information in different ways, leading to different notions of behavioural equivalence. 
A typical example are the two processes 
\[P = c?x.H(x).\nil \qquad Q = c?x.X(x).\nil\]
(here written in the syntax of qCCS). Both processes receive a qubit, apply a unitary transformation on it, and then terminate. The first process applies an Hadamard transformation, while the second applies a $X$ transformation, resulting in a different quantum state. The discriminating question is, should $P$ and $Q$ be considered bisimilar? That is, do $P$ and $Q$ express the same observable behaviour, and are therefore indistinguishable? 

The answer depends on the exact notion of \textit{observable property}, which varies between the proposed calculi. In QPAlg and CQP, a qubit is observable only when it is sent, as it is the case for "classical" value passing or name passing calculi. In this setting, $P$ and $Q$ are indeed bisimilar, because the qubit they modified is not visible to an external observer. In qCCS instead, after a process has terminated, the qubits it has operated on become observable, so when $P$ and $Q$ reach $\nil$, they will have left the qubit in two different states, and this sufficies to refutate bisimilarity in qCCS. This discrepancy is due to a sort of "ambiguity", present in the syntax of all the calculi proposed up to now, on what happens to the qubits after the process termination.
 
 Another crucial notion when defining behavioural equivalence is how to confront qubit values. In a classical process calculus, we can say that $c!0.\nil$ is not bisimilar to $c!1.\nil$ because they send two different \textit{values}. In a quantum setting it is difficult to talk about the value of a single qubit, because the qubit may be entangled.  Take for example the two processes 
 \[R = Set_{\Phi}(q_1, q_2).c!q_1.c!q_2 \qquad 
   S = Set_{\Psi}(q_1, q_2).c!q_1.c!q_2\]
where $R$ and $S$ set the value of the two qubits to different entangled states, and send them.
Quantum theory tells us that entangled states are distinguishable only when taken as a whole, as a compound 2-qubit system (for example, we could take $\Phi = \oost\ket{00} + \oost\ket{11}$ and $\Psi = \oost\ket{01} + \oost\ket{10}$, as it will be explained in the next chapter). In the presence of entanglement, one cannot describe the state of just qubit $q_1$ without loosing some information. For this reason, if the values of qbits are considered only one at the time, we lose some discriminating power allowed by quantum theory.

On the opposite side, quantum theory comes also with negative results about distinguishability of probabilistic mixtures (i.e. distributions) of quantum states. Also in this case, the capability of the observer to discriminate between different entities must be considered carefully, and is addressed or ignored in different calculi.
 
Summarizing, different calculi deals with quantum features in different ways. In QPAlg the value of a qubit is defined to ignore entanglement, and so the two processes $R$ and $S$ above are deemed bisimilar. In qCCS, the "environment", i.e. the state of all visible qubits, is considered when comparing two processes, thus $R$ and $S$ are not bisimilar. In CQP there is a similar notion of environment, but takes into account also the limited discriminating capability of an external observer for probabilistic mixtures of quantum states, naturally arising upon measurements.
 
\subsection*{Linear qCCS: a unifying approach}
The main objective of this thesis is to introduce Linear qCCS, and use it to investigate the different notions of behavioural equivalence of quantum systems. Linear qCCS (lqCCS) is designed to resolve the ambiguities and discrepancies of the previous proposals, and to identify the most adequate behavioural equivalence relation.
%without relying on any preconceived notion of observable property.

lqCCS is a (asynchronous) version on qCCS, equipped with a linear type system to regulate exclusively quantum communication. Indeed, due to the No-Cloning Theorem, once a qubit is sent on a channel, the sender process cannot use it anymore. The type system is inspired by the affine one in CQP, but it enforces a stricter policy on qubit input and outputs. In lqCCS each qubit used by a process must be sent \textit{exactly} once, while in CQP it must be sent at most one. This means that the two processes $P$ and $Q$ seen before are not well typed in lqCCS, while $P', Q', P'', Q''$ are.
\begin{align*}
P' &= c?x.H(x).c!x &  Q' &= c?x.X(x).c!x \\
P'' &= c?x.H(x).discard(x) &  Q'' &= c?x.X(x).discard(x)
\end{align*}
The $discard(q)$ action is typed the same as a send action $c!q$, but when it comes to bisimilarity, $q$ is not considered a visible qubit. For example, in (standard) qCCS $discard(q)$ can be implemented with channel restrictions, like $c!q\setminus c$.

Thanks to its linear type system, lqCCS forces the designer to make an explicit choice on what happens to the qubits at the end of a computation. By writing processes like $P'$ or $Q'$, the qubit are made visible, thus the two processes are not bisimilar. When writing processes like $P''$ and $Q''$, the qubit is not visible, and the processes are bisimilar. Note that when deciding to use a discard or a send action, we can switch between a behaviour a-la QPAlg/CQP (where $P$ is bisimilar to $Q$), and a-la qCCS (where they are not).

Having solved the ambiguity on when a qubit should be visible or not, the only relevant detail is how describe the quantum state. This allows us to compare directly the different bisimilarity relations presented in the literature, and to determine which are the crucial  quantum-related details that makes a difference when defining a bisimilarity.  

Since there  is no unique and accepted notion of "observable property" of quantum processes, each definition of labeled bisimilarity makes some assumption on which are the properties to be checked when comparing two processes. Instead we opted for a (probabilistic) \textit{saturated bisimilarity}, which in turn is based on an Unlabeled Transition System, and embodies both bisimilarity and contextual equivalence. Two processes $P$ and $Q$ are saturated bisimilar if they express the same atomic observable properties (called \textit{barbs}), and when put inside the same context they evolve to bisimilar processes. As barbs we take the property of "being able to send on a specific channel", which  is a standard choice for barbs. Notice that in this way we made no a-priori choice on quantum values representation. In saturated bisimilarity, in fact, it is charge of the the context to distinguish. For example, $P'$ and $Q'$ seen before are distinguished by a context that receives the modified qubit, measures it and is able to tell the difference.

A technical advantage of such saturated bisimilarity is that, instead of adding requirements to guarantee that the labelled bisimilarity is a congruence, we start from a relation that is a congruence by definition, and explore which are the properties that can be observed by an arbitrary context.


\subsection*{The unexpected inadequacy of probabilistic bisimilarity}

With probabilistic saturated bisimilarity we can recover the same (labelled) bisimilarity notion of qCCS, but not the one presented in the latest works on CQP. The main difference between qCCS and CQP bisimilarity is in the treatment of measurement operatorsand the resulting probabilistic behaviour.

Consider the two processes 
\[P = c?x.M_{01}(x).c!x \qquad Q = c?x.M_\pm(x).c!x\]
Both processes receive a qubit, measure it and then send the measured qubit. Note that measurement are of two different kind. As it will be clear later, this causes the resulting state of the sent qubits to differ. $P$ measures the qubit in the $01$ or computational basis, so will send two possible qubits, $\kz$ or $\ko$, each with a certain probability. $Q$ measures in the $\pm$ or diagonal basis, so will send two possible qubits, $+$ and $-$, each with a certain probability. There is an input for which $P$ will evolve in a symmetric \textit{distribution} of states, i.e. will send a qubit with state $\kz$ with probability $0.5$, or a qubit with state $\ko$ with probability $0.5$. Similarly, with the same input state, $Q$ will evolve in a symmetric distribution of sending $\kpl$ and $\km$.

When dealing with distributions the usual notion of  probabilistic bisimilarity says that two distribution are bisimilar when they assign the same probability to bisimilar processes. So, according to this definition, the processes $P$ and $Q$ are not bisimilar in qCCS and the same holds for probabilistic saturated bisimilarity in lqCCS.

Quantum theory, however, tells that it is not possible to distinguish a source $S_P$ sending half of the time $\kz$ and half of the time $\ko$ from a source $S_Q$ sending half of the time $\kpl$ and half of the time $\km$. This interesting property is due to the inherent limitations of the observer, that to discriminate the two sources can only try to perform measurements, and the obtained probabilistic distributions coincides for every choice of measurement. An an example, assume we measure the qubits from $S_P$ in the computational basis, the result will be half of the time $\kz$ and half of the time $\ko$, as the measurement in the computational basis does nothing on a qubit already in the computational basis. Similarly, when measuring a qubit coming from $S_Q$, a $\kpl$ qubit would decay in $\kz$ or $\ko$, each with the same probability, and the same happens for $\km$. Hence, the two sources appear the same for an external observer performing the measurement on the computational basis, and the same happens for all the basis. 
In accordance with this peculiar quantum feature, in CQP the processes $P$ and $Q$ are considered bisimilar. This is obtained by
defining observable properties directly on distributions, instead of simply lifting bisimilarty of processes.

We identify the well established notion of Larsen-Skou \cite{larsenBisimulationProbabilisticTesting1991} probabilistic bisimulation  as the cause of this undesired behaviour of qCCS, and propose a novel notion of \textit{quantum saturated bisimilarity} that represents more correctly the observable properties of probabilistic distribution of quantum states. To obtain this result, we model the undishtingishability results of quantum theory as an equivalence relation between distributions of states and processes.  
%Thanks to this equivalence relation, we relax the conditions of saturated bisimilarity, asking that when two distribution $\Delta$ and $\Theta$ are bisimilar, they are not necessarily Larsen-Skou bisimilar, but there are some equivalent distribution $\Delta'$ and $\Theta'$ that are Larsen-Skou bisimilar.
Thanks to this equivalence relation, we relax the conditions of saturated bisimilarity, allowing to change the "actual" distribution with an equivalent one to match the behaviour of a bisimilar distribution.
 Using quantum saturated bisimilarity we can recover many results obtained for CQP bisimilarity, although in a different and arguably more standard transition system.
 
 \subsection*{A minimal quantum process calculus}
 
The usual notion of probabilistic bisimilarity seems not well suited to treat distributions of configurations containing quantum values, even if quantum systems have some obvious probabilistic features. Given that a well established notion of behavioural equivalence for quantum processes has not emerged yet, we have decided to pursue a foundational approach, investigating the topic on a Minimal Quantum Process Algebra (mQPA).
Working with a minimal process algebra allows us to omit all the irrelevant classical details, and to target the quantum aspects in isolation.
In addition to a minimal set of classical features, mQPA has unitary transformations (representing the evolution of isolated quantum states),
and a novel operator for combining processes, called projective sum.
Similarly to what happens in a probabilistic process algebra, where a probabilistic sum operator produces a distribution
of states, the projective sum produces a quantum distribution of states, i.e. a probabilistic distribution that depends on the quantum value
to be measured.
The state of the qubits is not kept along the process, and is not updated in a small-step fashon as the process evolves.
On the countrary, the operational semantics of a mQPA is defined as implicitly parametric on the quantum state.
This approach is peculiar, as it resamble none of the current proposals (to the best of our knowledge).
A bisimilarity then is defined in a fairly standard way, and is shown to work well with the previous problematic examples
 